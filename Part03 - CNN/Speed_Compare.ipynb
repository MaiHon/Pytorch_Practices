{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source from [Here](https://ynd.co/blog/tensorflow-vs-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pytorch_lightning\n",
    "\n",
    "# for tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# other usefull library\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the transformer for put the data to the tensor\n",
    "transforms = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# load the train data - if not exists download it\n",
    "train_dataset_py = torchvision.datasets.FashionMNIST(root='./Fashion_MNIST/',\n",
    "                                             train=True, \n",
    "                                             transform=transforms,\n",
    "                                             download=True)\n",
    "\n",
    "test_dataset_py = torchvision.datasets.FashionMNIST(root='./Fashion_MNIST/',\n",
    "                                             train=False, \n",
    "                                             transform=transforms,\n",
    "                                             download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "#split to test and train\n",
    "(train_images_tf, train_labels_tf), (test_images_tf, test_labels_tf) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tf = train_images_tf / 255.0\n",
    "test_images_tf = test_images_tf / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowPytorch(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "def imshowTensorFlow(img):\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAR3ElEQVR4nO3dX4zV9ZnH8c/jf0BEEfkrEVpRaTYurogSienaapQbrBqsFxs1ujSmJm3ixRr3osYLNbptsxfahKopXbs2TVqjxn9lTRN3g1ZGZQGZbQGVCPJPQeWfwuCzF3M0U53f84znd/7p9/1KyMycZ75zvnNmPpwz5znf79fcXQC+/o7o9gQAdAZhBwpB2IFCEHagEIQdKMRRnbwyM+Opf6DN3N2Gu7zWPbuZXWZmfzGzDWZ2W52vBaC9rNk+u5kdKemvki6RtFnSSknXuvu6YAz37ECbteOefZ6kDe7+hrsflPRbSYtqfD0AbVQn7NMkvT3k482Ny/6GmS0xsz4z66txXQBqavsTdO6+VNJSiYfxQDfVuWffImn6kI9PbVwGoAfVCftKSbPMbKaZHSPp+5KeaM20ALRa0w/j3X3AzG6R9JykIyU97O6vt2xmAFqq6dZbU1fG3+xA27XlRTUAvjoIO1AIwg4UgrADhSDsQCEIO1AIwg4UgrADhSDsQCEIO1AIwg4UgrADhSDsQCE6upU0Os9s2AVQn6m76nHs2LFhfcGCBZW1Z555ptZ1Z9/bkUceWVkbGBiodd11ZXOPNPsz454dKARhBwpB2IFCEHagEIQdKARhBwpB2IFC0Gf/mjviiPj/88OHD4f1008/PazfdNNNYf3AgQOVtX379oVjP/roo7D+8ssvh/U6vfSsD57drtn4OnOLXj8Q/Ty5ZwcKQdiBQhB2oBCEHSgEYQcKQdiBQhB2oBD02b/mop6slPfZL7744rD+3e9+N6xv3ry5snbssceGY0ePHh3WL7nkkrD+4IMPVta2b98ejs3WjGe3W+b444+vrH3yySfh2P379zd1nbXCbmZvSdoj6bCkAXefW+frAWifVtyz/6O7v9uCrwOgjfibHShE3bC7pD+a2StmtmS4TzCzJWbWZ2Z9Na8LQA11H8YvcPctZjZR0nIz+z93f2HoJ7j7UklLJcnM6u1uCKBpte7Z3X1L4+0OSY9JmteKSQFovabDbmZjzGzsp+9LulTS2lZNDEBr1XkYP0nSY411u0dJ+k93f7Yls0LLHDx4sNb48847L6zPmDEjrEd9/mxN+HPPPRfWzznnnLB+7733Vtb6+uKnkNasWRPW+/v7w/q8efGD3Oh2XbFiRTj2xRdfrKzt3bu3stZ02N39DUl/3+x4AJ1F6w0oBGEHCkHYgUIQdqAQhB0ohNU9svdLXRmvoGuLaNvi7OebLRON2leSdOKJJ4b1Q4cOVdaypZyZlStXhvUNGzZU1rKWZLYV9OTJk8N69H1L8dyvvvrqcOwDDzwQft0PP/xw2Mlzzw4UgrADhSDsQCEIO1AIwg4UgrADhSDsQCHos/eArKdbR/bzfemll8J6toQ1E31v2bHFdZfnRkc+Zz3+1157LayvX78+rGff2+WXX15ZmzlzZjh22rRpYd3d6bMDJSPsQCEIO1AIwg4UgrADhSDsQCEIO1AIjmzuAZ18rcPn7d69O6xPmTIlrB84cCCsR8cyH3300eHY6FhjKe6jS9KoUaMqa1mffcGCBWF9/vz5YT3bJnvixImVtWefbc+O7NyzA4Ug7EAhCDtQCMIOFIKwA4Ug7EAhCDtQCPrshRs9enRYj45clvJ+8v79+ytrH3zwQTh2165dYT1bax/10rM9BLLvK7vdDh8+HNajuU2fPj0c26z0nt3MHjazHWa2dshl481suZmtb7w9qS2zA9AyI3kY/ytJl33ustskPe/usyQ93/gYQA9Lw+7uL0j6/OOpRZKWNd5fJumKFs8LQIs1+zf7JHff2nh/m6RJVZ9oZkskLWnyegC0SO0n6Nzdo40k3X2ppKUSG04C3dRs6227mU2RpMbbHa2bEoB2aDbsT0i6rvH+dZIeb810ALRL+jDezB6V9G1JE8xss6SfSLpH0u/M7EZJmyQtbuckv+7q9nyjnm62Jnzq1KlhPVsznu3tfswxxzQ9dt++fWF93LhxYf29996rrGV98mjekrR3796wfsIJJ4T11atXV9ayn9ncuXMra+vWrauspWF392srSt/JxgLoHbxcFigEYQcKQdiBQhB2oBCEHSgES1x7QLaVdLbMNGq9XXPNNeHYbKvoHTvi10sdd9xxYT1ayjlmzJhwbLbUM2vdRdtYHzp0KBx71FFxNLLv++STTw7r999/f2Vtzpw54dhoblEbl3t2oBCEHSgEYQcKQdiBQhB2oBCEHSgEYQcKYZ08LpidaoaX9XQHBgaa/trnn39+WH/qqafCerbENVt+G/XZ6x7JHC1hleIjobPjorPXAGRHXWei7+2+++4Lxz7yyCNh3d2HbbZzzw4UgrADhSDsQCEIO1AIwg4UgrADhSDsQCG+UuvZo7W6dY8WzrZzjtY/R73kkajTR888/fTTYT3brvnAgQNhPdtyOXodx86dO8Ox2c80W1OerVmvMzb7mWdzP/vssytr2VHWzeKeHSgEYQcKQdiBQhB2oBCEHSgEYQcKQdiBQvRUn73O/ujt7FW320UXXRTWr7rqqrB+4YUXVtayPnm2Jjzro2dr8aOf2f79+8Ox2e9DtC+8FPfhs30csrllststen3DlVdeGY598sknm5pTes9uZg+b2Q4zWzvksjvMbIuZrWr8W9jUtQPomJE8jP+VpMuGufzn7j6n8S9+mRaArkvD7u4vSNrVgbkAaKM6T9DdYmarGw/zT6r6JDNbYmZ9ZtZX47oA1NRs2H8h6ZuS5kjaKumnVZ/o7kvdfa67z23yugC0QFNhd/ft7n7Y3T+R9EtJ81o7LQCt1lTYzWzoOb/fk7S26nMB9IZ033gze1TStyVNkLRd0k8aH8+R5JLekvQDd9+aXlkX940fP358WJ86dWpYP+OMMypr2RnnWd/0zDPPDOt19m7P1mWPGjUqrL/zzjthPdt/Peo3Z2eYZ+evjx49OqyvWLGispbtWZ+99iFbz56tSY9ut+3bt4djZ8+eHdar9o1PX1Tj7tcOc/FD2TgAvYWXywKFIOxAIQg7UAjCDhSCsAOF6Kkjm+fPnx+Ov/POOytrp5xySjj2xBNPDOvRUkwpXm75/vvvh2Oz5bdZCylrQUXbYGdLXPv7+8P64sWLw3pfX/wq6LFjx1bWTjqp8lXWkqQZM2aE9cwbb7xRWYvmJUl79uwJ69kS2KylGbX+TjjhhHBs9vvCkc1A4Qg7UAjCDhSCsAOFIOxAIQg7UAjCDhSi4332qF/94osvhuOjZahZLzvro9fZOjjb8jjrddc1bty4ytqECRPCsddff31Yv/TSS8P6zTffHNajJbLZ0t0333wzrEd9dEmaNWtWZa3u8tpsaW/Wx4+W/ma/q6eddlpYp88OFI6wA4Ug7EAhCDtQCMIOFIKwA4Ug7EAhOtpnnzBhgi9atKiyfvfdd4fjN27cWFnLtgbO6tnxv5Gs5xr1wSXp7bffDuvZds7RWv5om2lJmjx5cli/4oorwnp0LLIkzZw5s7I2ZsyYcOy5555bqx5971kfPbvdsiOZM9EeBNnv0wUXXFBZ27Ztmw4ePEifHSgZYQcKQdiBQhB2oBCEHSgEYQcKQdiBQqSnuLbSwMBAeBxt1m+O9tPO1kZnXzvrw0d91Wyf7127doX1TZs2hfVsbtF6+ex2yfYBeOyxx8L6mjVrwnq093t2jHbWC8/264+Oq87WjNddz54d6Rz12bMefnR8eHSbpPfsZjbdzP5kZuvM7HUz+1Hj8vFmttzM1jfexjv+A+iqkTyMH5B0q7t/S9IFkn5oZt+SdJuk5919lqTnGx8D6FFp2N19q7u/2nh/j6R+SdMkLZK0rPFpyyTFr6sE0FVf6gk6M5sh6RxJf5Y0yd23NkrbJE2qGLPEzPrMrC/7OwhA+4w47GZ2vKTfS/qxu384tOaDq2mGXVHj7kvdfa67z627eABA80YUdjM7WoNB/427/6Fx8XYzm9KoT5G0oz1TBNAKaevNBnsED0nqd/efDSk9Iek6Sfc03j6efa2DBw9qy5YtlfVsuW3UPsuWS2ZbKmdtnHfffbeytnPnznDsUUfFN3O2vDZr80TLTLMtjbOlnNH3LUmzZ88O6/v27ausZe3Q3bt3h/XsdovmHrXlpLwlmY3PjmyOlhZ/8MEH4dg5c+ZU1tauXVtZG0mf/UJJ/yRpjZmtalx2uwZD/jszu1HSJknxQd4AuioNu7v/j6SqVwB8p7XTAdAuvFwWKARhBwpB2IFCEHagEIQdKERHl7geOHBAq1atqqxnyylvuOGGylq23XJ2vG+2FDRaZpq9MjDbbjkbnx0J/fHHH1fWsqWc2WsbsqOst23bFtajpZ7Z3LLXJ9T5mdVdPltnea0U9/Gj7bclhcvEo+vlnh0oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUJ09MhmM6t1ZQsXLqys3XrrreHYSZOG3TXrM9ma9KivmvWLsz551mfP+s3R14+2LJbyPnu2lj6rR99bNjabeyYaH/WqRyL7mWVbSUfr2VevXh2OXbw4Xk3u7hzZDJSMsAOFIOxAIQg7UAjCDhSCsAOFIOxAITreZ4/2Kc96k3VcfPHFYf2uu+4K6xMnTqysjRs3Lhyb7c2e9eGzPnvU58961Vm/Ofv9iM4BkOKf6d69e8Ox2e2SieaerTfP1vFnP9Ply5eH9f7+/sraihUrwrEZ+uxA4Qg7UAjCDhSCsAOFIOxAIQg7UAjCDhQi7bOb2XRJv5Y0SZJLWuru/25md0j6Z0mfLgS/3d2fTr5W55r6HXTWWWeF9VNOOSWsZ+eQn3rqqWF906ZNlbVsf/SNGzeGdXz1VPXZR3JIxICkW939VTMbK+kVM/v0FQM/d/d/a9UkAbTPSM5n3yppa+P9PWbWL2lauycGoLW+1N/sZjZD0jmS/ty46BYzW21mD5vZSRVjlphZn5n11ZopgFpGHHYzO17S7yX92N0/lPQLSd+UNEeD9/w/HW6cuy9197nuPrcF8wXQpBGF3cyO1mDQf+Puf5Akd9/u7ofd/RNJv5Q0r33TBFBXGnYbXDb1kKR+d//ZkMunDPm070la2/rpAWiVkbTeFkj6b0lrJH26XvF2Sddq8CG8S3pL0g8aT+ZFX+tr2XoDeklV6+0rtW88gBzr2YHCEXagEIQdKARhBwpB2IFCEHagEIQdKARhBwpB2IFCEHagEIQdKARhBwpB2IFCEHagECPZXbaV3pU0dN/jCY3LelGvzq1X5yUxt2a1cm6nVRU6up79C1du1tere9P16tx6dV4Sc2tWp+bGw3igEIQdKES3w760y9cf6dW59eq8JObWrI7Mrat/swPonG7fswPoEMIOFKIrYTezy8zsL2a2wcxu68YcqpjZW2a2xsxWdft8usYZejvMbO2Qy8ab2XIzW994O+wZe12a2x1mtqVx260ys4Vdmtt0M/uTma0zs9fN7EeNy7t62wXz6sjt1vG/2c3sSEl/lXSJpM2SVkq61t3XdXQiFczsLUlz3b3rL8Aws4sk7ZX0a3f/u8Zl90ra5e73NP6jPMnd/6VH5naHpL3dPsa7cVrRlKHHjEu6QtL16uJtF8xrsTpwu3Xjnn2epA3u/oa7H5T0W0mLujCPnufuL0ja9bmLF0la1nh/mQZ/WTquYm49wd23uvurjff3SPr0mPGu3nbBvDqiG2GfJuntIR9vVm+d9+6S/mhmr5jZkm5PZhiThhyztU3SpG5OZhjpMd6d9Lljxnvmtmvm+PO6eILuixa4+z9IulzSDxsPV3uSD/4N1ku90xEd490pwxwz/plu3nbNHn9eVzfCvkXS9CEfn9q4rCe4+5bG2x2SHlPvHUW9/dMTdBtvd3R5Pp/ppWO8hztmXD1w23Xz+PNuhH2lpFlmNtPMjpH0fUlPdGEeX2BmYxpPnMjMxki6VL13FPUTkq5rvH+dpMe7OJe/0SvHeFcdM64u33ZdP/7c3Tv+T9JCDT4jv1HSv3ZjDhXz+oak/238e73bc5P0qAYf1h3S4HMbN0o6WdLzktZL+i9J43tobv+hwaO9V2swWFO6NLcFGnyIvlrSqsa/hd2+7YJ5deR24+WyQCF4gg4oBGEHCkHYgUIQdqAQhB0oBGEHCkHYgUL8P7BRN36A0fU1AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a data loader - I will use it for the training\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset_py,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=False, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset_py,\n",
    "                                           batch_size=32, \n",
    "                                           shuffle=False, num_workers=2)\n",
    "                                           \n",
    "data_iter = iter(train_loader)\n",
    "images, label = data_iter.next()\n",
    "imshowPytorch(torchvision.utils.make_grid(images[0]))\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imshowTensorFlow(train_images_tf[0])\n",
    "\n",
    "print(train_labels_tf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, num_of_class):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7 * 7 * 32, num_of_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltf = keras.Sequential([\n",
    "    keras.layers.Conv2D(input_shape=(28,28,1), filters=16, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    keras.layers.Conv2D(32, kernel_size=5, strides=1, padding=\"same\", activation=tf.nn.relu),\n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=1568, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelpy = NeuralNet(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(modelpy.parameters())\n",
    "\n",
    "modelpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 16)        416       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 28, 28, 16)        64        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        12832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                15690     \n",
      "=================================================================\n",
      "Total params: 29,130\n",
      "Trainable params: 29,034\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "modeltf.compile(optimizer=keras.optimizers.Adam(), \n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "modeltf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_modelParams(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29034\n"
     ]
    }
   ],
   "source": [
    "print(count_modelParams(modelpy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: loss: 0.38715100294152893\n",
      "step 1: loss: 0.2805996476908525\n",
      "step 2: loss: 0.24465357029040655\n",
      "step 3: loss: 0.2210026885519425\n",
      "step 4: loss: 0.20166135769983132\n",
      "step 5: loss: 0.18459562652210396\n",
      "step 6: loss: 0.16994346136103075\n",
      "step 7: loss: 0.15696369028737148\n",
      "step 8: loss: 0.145061860999465\n",
      "step 9: loss: 0.13538551735704143\n",
      "CPU times: user 19min 30s, sys: 37.2 s, total: 20min 7s\n",
      "Wall time: 8min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "modelpy.train()\n",
    "for e in range(10):\n",
    "    # define the loss value after the epoch\n",
    "    losss = 0.0\n",
    "    number_of_sub_epoch = 0\n",
    "    \n",
    "    # loop for every training batch (one epoch)\n",
    "    for images, labels in train_loader:\n",
    "        #create the output from the network\n",
    "        out = modelpy(images)\n",
    "        # count the loss function\n",
    "        loss = criterion(out, labels)\n",
    "        # in pytorch you have assign the zero for gradien in any sub epoch\n",
    "        optim.zero_grad()\n",
    "        # count the backpropagation\n",
    "        loss.backward()\n",
    "        # learning\n",
    "        optim.step()\n",
    "        # add new value to the main loss\n",
    "        losss += loss.item()\n",
    "        number_of_sub_epoch += 1\n",
    "    print(\"step {}: loss: {}\".format(e, losss / number_of_sub_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_tf = train_images_tf.reshape(train_images_tf.shape[0], \n",
    "                                          train_images_tf.shape[1],\n",
    "                                          train_images_tf.shape[2], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 37s 615us/sample - loss: 0.4046 - accuracy: 0.8584\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 36s 601us/sample - loss: 0.2856 - accuracy: 0.8984\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 36s 595us/sample - loss: 0.2495 - accuracy: 0.9098\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 38s 636us/sample - loss: 0.2240 - accuracy: 0.9179\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 37s 613us/sample - loss: 0.2036 - accuracy: 0.9260\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 37s 611us/sample - loss: 0.1880 - accuracy: 0.9307\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 36s 597us/sample - loss: 0.1724 - accuracy: 0.9370\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 36s 594us/sample - loss: 0.1616 - accuracy: 0.9403\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 37s 615us/sample - loss: 0.1483 - accuracy: 0.9467\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 36s 604us/sample - loss: 0.1401 - accuracy: 0.9491\n",
      "CPU times: user 15min 39s, sys: 1min 40s, total: 17min 19s\n",
      "Wall time: 6min 4s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x134ec7e50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "modeltf.fit(train_images_tf, train_labels_tf, epochs=10, batch_size=32, max_queue_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1075, grad_fn=<NllLossBackward>)\n",
      "<NllLossBackward object at 0x16bcea890>\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    out = modelpy(images)\n",
    "    \n",
    "    loss = criterion(out, labels)\n",
    "    print(loss)\n",
    "    print(loss.grad_fn)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.multiprocessing import Process, Pool, set_start_method\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    set_start_method('spawn')\n",
    "except RuntimeError:\n",
    "    print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = torch.nn.Conv2d(32, 32, 3, 1, 1)\n",
    "\n",
    "inp = np.random.randn(2, 32, 128, 128)\n",
    "inp = torch.Tensor(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.24 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 -r 1 conv2d(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "conv2d_tf = tf.keras.layers.Conv2D(32, 3, 1, 'same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = np.random.randn(2, 128, 128, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.22 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 -r 1 conv2d_tf(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
